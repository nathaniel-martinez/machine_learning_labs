{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 6: Ensemble methods\n",
    "\n",
    "This week we have covered a variety of ensemble methods, summarized here:\n",
    "\n",
    "https://chatgpt.com/share/671bac71-3aac-8002-a19d-047f0a97c619\n",
    "\n",
    "In this notebook please complete the following task.\n",
    "\n",
    "**Design your own ensemble method**\n",
    "\n",
    "For this task you should create a markdown cell that concisely but completely describes your idea.\n",
    "Following that, there should be a python implementation of your idea. \n",
    "You should run your algorithm on a real dataset and compare the performance to a random forest and some other well known ensemble approaches with default parameters (e.g. adaboost, gradient boost). \n",
    "\n",
    "*I want to see a nicely formatted table in which rows correspond to classifiers (yours included, at least 3 rows) and columns correspond to training and test accuracy of the models on the data.*\n",
    "\n",
    "Then answer these questions:\n",
    "* Is there reason to suppose that your approach would work better on certain types of data?\n",
    "* Can you algorithm be parallelized for training or prediction?\n",
    "\n",
    "Your algorithm should be fundamentally different from the algorithms we have seen, but could be a tweak or combination of those existing ideas. \n",
    "\n",
    "Motivating thoughts:\n",
    "\n",
    "* Can you think of a novel way to enhance ensemble diversity of opinion?\n",
    "* Can you think of a novel way for models to iteratively correct the mistakes of the previous models (as in boosting)?\n",
    "\n",
    "As always this assignment is partly a test of your ability to **communicate**. Write the appropriate amount of prose (not too much, not too little) and strive for clarity. Provide tables and visualizations where appropriate. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name random forest\n",
      "Model train score 1.0\n",
      "Model test score 0.5810055865921788\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spot/College/Machine_Learning/machine-learning-labs/lib/python3.11/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name adaboost\n",
      "Model train score 0.8553370786516854\n",
      "Model test score 0.7541899441340782\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "X = df.drop([\"Survived\", \"Name\"], axis=1)\n",
    "y = df[\"Survived\"]\n",
    "\n",
    "categorical_fields = [\"Pclass\", \"Sex\", \"Cabin\", \"Embarked\", \"Ticket\"]\n",
    "numerical_fields = [field for field in X.columns if field not in categorical_fields]\n",
    "\n",
    "# Transform last name into numerial ordering of names in alphabetical order\n",
    "# My guess is that people where chosen by last name in order leave on a life boat?\n",
    "df[\"Last_Name\"] = df[\"Name\"].str.extract(r'^(.*?)(?=,)')\n",
    "sorted_names = df['Last_Name'].sort_values().reset_index(drop=True)\n",
    "last_name_order = {last_name: i + 1 for i, last_name in enumerate(sorted_names)}\n",
    "X[\"Last_Name_Rank\"] = df[\"Last_Name\"].map(last_name_order)\n",
    "\n",
    "# Setting N/A values of Age to the mean\n",
    "X[\"Age\"] = X[\"Age\"].fillna(X[\"Age\"].mean())\n",
    "\n",
    "# Setting Unkown Cabins to a single category. Maybe people with unkown cabins were less documented and more likley to die?\n",
    "X['Cabin'] = X['Cabin'].replace('', \"Unknown\")\n",
    "X[\"Cabin\"] = X[\"Cabin\"].fillna(\"Unknown\")\n",
    "\n",
    "\n",
    "# Setting Embarked nan to Unkown\n",
    "X[\"Embarked\"] = X[\"Embarked\"].fillna(\"Unkown\")\n",
    "\n",
    "# Our transformations on numerical and categorical fields\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_fields),\n",
    "        ('cat', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), categorical_fields)\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "X_train = preprocessing_pipeline.fit_transform(X_train)\n",
    "X_test = preprocessing_pipeline.fit_transform(X_test)\n",
    "\n",
    "#My custom ensemble!\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "adaboost = AdaBoostClassifier()\n",
    "\n",
    "def results(model, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"Model name {model_name}\")\n",
    "    print(f\"Model train score {model.score(X_train, y_train)}\")\n",
    "    print(f\"Model test score {model.score(X_test, y_test)}\")\n",
    "    print(\"\")\n",
    "\n",
    "results(random_forest, \"random forest\")\n",
    "results(adaboost, \"adaboost\")\n",
    "#custom_ensemble = CustomEnsemble()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model almost_blind_trees\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 118\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mscore(X_test,\u001b[38;5;250m \u001b[39my_test)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 118\u001b[0m \u001b[43mprint_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43malmost_blind_trees\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malmost_blind_trees\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 112\u001b[0m, in \u001b[0;36mprint_scores\u001b[0;34m(model, model_name)\u001b[0m\n\u001b[1;32m    110\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mscore(X_test,\u001b[38;5;250m \u001b[39my_test)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 91\u001b[0m, in \u001b[0;36mAlmost_Blind_Trees.score\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     88\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;66;03m# Np arrray to hold our predictions that we will \u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (x_instance, y_val) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(X, y)):\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# Our prediction for each individual y instance\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[43m[\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_instance\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malmost_blind_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prediction \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m:\n\u001b[1;32m     93\u001b[0m         predictions[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[18], line 91\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     88\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;66;03m# Np arrray to hold our predictions that we will \u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (x_instance, y_val) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(X, y)):\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# Our prediction for each individual y instance\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([weight \u001b[38;5;241m*\u001b[39m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_instance\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m weight, tree \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malmost_blind_trees)])\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prediction \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m:\n\u001b[1;32m     93\u001b[0m         predictions[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/College/Machine_Learning/machine-learning-labs/lib/python3.11/site-packages/sklearn/tree/_classes.py:529\u001b[0m, in \u001b[0;36mBaseDecisionTree.predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict class or regression value for X.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \n\u001b[1;32m    508\u001b[0m \u001b[38;5;124;03mFor a classification model, the predicted class for each sample in X is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;124;03m    The predicted classes, or the predict values.\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    528\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 529\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m    531\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/College/Machine_Learning/machine-learning-labs/lib/python3.11/site-packages/sklearn/tree/_classes.py:489\u001b[0m, in \u001b[0;36mBaseDecisionTree._validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     force_all_finite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    497\u001b[0m     X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc\n\u001b[1;32m    498\u001b[0m ):\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/College/Machine_Learning/machine-learning-labs/lib/python3.11/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/College/Machine_Learning/machine-learning-labs/lib/python3.11/site-packages/sklearn/utils/validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1061\u001b[0m     )\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/College/Machine_Learning/machine-learning-labs/lib/python3.11/site-packages/sklearn/utils/validation.py:111\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput contains NaN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# We need only consider float arrays, hence can early return for all else.\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreal floating\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomplex floating\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# First try an O(n) time, O(1) space solution for the common case that\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# everything is finite; fall back to O(n) space `np.isinf/isnan` or custom\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Cython implementation to prevent false positives and provide a detailed\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# error message.\u001b[39;00m\n",
      "File \u001b[0;32m~/College/Machine_Learning/machine-learning-labs/lib/python3.11/site-packages/sklearn/utils/_array_api.py:428\u001b[0m, in \u001b[0;36m_NumPyAPIWrapper.isdtype\u001b[0;34m(self, dtype, kind)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misdtype\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype, kind):\n\u001b[0;32m--> 428\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43misdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/College/Machine_Learning/machine-learning-labs/lib/python3.11/site-packages/sklearn/utils/_array_api.py:196\u001b[0m, in \u001b[0;36misdtype\u001b[0;34m(dtype, kind, xp)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a boolean indicating whether a provided dtype is of type \"kind\".\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03mIncluded in the v2022.12 of the Array API spec.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;124;03mhttps://data-apis.org/array-api/latest/API_specification/generated/array_api.isdtype.html\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kind, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28many\u001b[39m(_isdtype_single(dtype, k, xp\u001b[38;5;241m=\u001b[39mxp) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kind)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _isdtype_single(dtype, kind, xp\u001b[38;5;241m=\u001b[39mxp)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Source Code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "y = df['Survived']\n",
    "# For some reason Sex colunm was needed to predict survivabillity. As such, I dropped it. PasasangerID also seems irrelevant\n",
    "X = df.drop(columns=['Survived', 'PassengerId', \"Sex\"])\n",
    "\n",
    "categorical_fields = ['Pclass', 'Embarked', 'Name', 'Cabin', 'Ticket']\n",
    "numerical_fields = [col for col in X.columns if col not in categorical_fields]\n",
    "\n",
    "# This transforms N/A values in the numerical fields to the mean of all the numerical field values\n",
    "numerical_transformer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# This first makes all N/A or None fields into a single category, and then applies On Hot Encoding on them\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# This is used to transform the X values with the transformations above\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_fields),\n",
    "        ('cat', categorical_transformer, categorical_fields)\n",
    "    ]\n",
    ")\n",
    "X = preprocessor.fit_transform(X)\n",
    "\n",
    "# Creating random forest\n",
    "random_forest = Pipeline([\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "#Creating adaboost\n",
    "adaboost = Pipeline([\n",
    "    ('classifier', AdaBoostClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=1),\n",
    "        n_estimators=100,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "#My Model!\n",
    "class Almost_Blind_Trees:\n",
    "    def __init__(self):\n",
    "        self.almost_blind_trees = []\n",
    "        self.weights = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        for i, (X_val, y_val) in enumerate(zip(X, y)):\n",
    "            X_instance = np.array([X_val]) #current instance of X\n",
    "            y_instance = np.array([y_val]) #current instance of y \n",
    "\n",
    "            # 29 other Random instances with repition in X and y\n",
    "            additional_indices = np.random.choice(y_train.shape[0], 4, replace=False)\n",
    "            additional_samples = X_train[additional_indices]\n",
    "            additional_targets = y_train.iloc[additional_indices]\n",
    "\n",
    "            # Concatentating these instances to train tree on them both\n",
    "            X_train_instance = np.vstack((X_instance, additional_samples))\n",
    "            y_train_instance = np.concatenate((y_instance, additional_targets))\n",
    "\n",
    "            tree = DecisionTreeClassifier()\n",
    "            tree.fit(X_train_instance, y_train_instance)\n",
    "            self.almost_blind_trees.append(tree)\n",
    "            \n",
    "            # Score our weak learner on the entire data set\n",
    "            self.weights.append(self.score_transform(tree.score(X, y)))\n",
    "            \n",
    "        # Apply a weight based on how succesful our weak learner was on the entire data set\n",
    "        total = sum(self.weights)\n",
    "        self.weights = [score / total for score in self.weights]\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        predictions = np.empty(y.shape[0]) # Np arrray to hold our predictions that we will \n",
    "        for i, (x_instance, y_val) in enumerate(zip(X, y)):\n",
    "            # Our prediction for each individual y instance\n",
    "            prediction = sum([weight * tree.predict([x_instance])[0] for weight, tree in zip(self.weights, self.almost_blind_trees)])\n",
    "            if prediction >= 0.5:\n",
    "                predictions[i] = 1\n",
    "            else:\n",
    "                predictions[i] = 0\n",
    "        # How many predictions are equal to the labels as a num from 0 - 1\n",
    "        return (np.sum(predictions == y) / y.shape[0])\n",
    "    \n",
    "    # Edit the score which weights are averaged on. We can adjust this to determine how fairly weights are assigned to models that do well vs those that dont\n",
    "    def score_transform(self, x):\n",
    "        return x**4\n",
    "\n",
    "\n",
    "almost_blind_trees = Almost_Blind_Trees()\n",
    "\n",
    "# Defining the test train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "def print_scores(model, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"For model {model_name}\")\n",
    "    print(f\"training score: {model.score(X_train, y_train)}\")\n",
    "    print(f\"Testing score: {model.score(X_test, y_test)}\")\n",
    "    print(\"\")\n",
    "    \n",
    "\n",
    "    \n",
    "print_scores(almost_blind_trees, \"almost_blind_trees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|            | Training Score         | Testing Score        |\n",
    "|------------|-------------------|------------------|\n",
    "| Random Forest | 1 | 0.6190476190476191 |\n",
    "| Adaboost Trees | 0.9417808219178082 | 0.6190476190476191 |\n",
    "| Custom Ensemble Model | 0.636986301369863 | 0.6349206349206349 ||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Is there reason to suppose that your approach would work better on certain types of data?\n",
    "    Can you algorithm be parallelized for training or prediction?\n",
    "    \n",
    "In viewing the training and testing results of Random Forests and Adaboost trees, I noticed immediatly that they were overfitting the training data to an extreme. As such, I wanted to create a Ensemble method based on trees that would prevent this overfitting from ocurring. As such, I created A Cusrom Ensemble Model that I would like to call Almost Blind Trees. I created this model based on the assumption that the data which I used had a simpler relationship between fields and labels then what the trees were picking up. For this project I used the famouse titanic data set and I believed that the survivability of a passanger would not be a relationship found by comparing multiple data point, and that it was a simpler relationship which could be detected withing a couple of data points. As such, my model consisted of several max depth 2 trees that were trained on three data points at the same time. These data points would then be used to determine how my model would interact with  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-labs",
   "language": "python",
   "name": "machine-learning-labs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
