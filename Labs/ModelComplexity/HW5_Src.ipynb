{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53f15af2",
   "metadata": {},
   "source": [
    "### [Chat GPT Link](https://chatgpt.com/share/671b402b-1db4-8007-9b31-8578daaa224a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8fdd1217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['baseline value', 'accelerations', 'fetal_movement',\n",
      "       'uterine_contractions', 'light_decelerations', 'severe_decelerations',\n",
      "       'prolongued_decelerations', 'abnormal_short_term_variability',\n",
      "       'mean_value_of_short_term_variability',\n",
      "       'percentage_of_time_with_abnormal_long_term_variability',\n",
      "       'mean_value_of_long_term_variability', 'histogram_width',\n",
      "       'histogram_min', 'histogram_max', 'histogram_number_of_peaks',\n",
      "       'histogram_number_of_zeroes', 'histogram_mode', 'histogram_mean',\n",
      "       'histogram_median', 'histogram_variance', 'histogram_tendency',\n",
      "       'fetal_health'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Stage 1A\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMacScalar\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "file_path = './fetal_health.xls'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "# Drop unrelated features\n",
    "unrelated_features = ['histogram_width',\n",
    "       'histogram_min', 'histogram_max', 'histogram_number_of_peaks',\n",
    "       'histogram_number_of_zeroes', 'histogram_mode', 'histogram_mean',\n",
    "       'histogram_median', 'histogram_variance', 'histogram_tendency']\n",
    "df.drop(columns = unrelated_features, inplace = True)\n",
    "\n",
    "X = df.drop(columns=['fetal_health'])\n",
    "y = df['fetal_health']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a86edcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage 1B\n",
    "\n",
    "# Split the data into training and testing sets, using 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#Aplying scaler transformation\n",
    "X_train_scl = scaler.fit_transform(X_train)\n",
    "X_test_scl = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "01a87e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage 1C\n",
    "\n",
    "# Degree 3 transformation\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X_p3 = poly.fit_transform(X)\n",
    "\n",
    "X_train_p3, X_test_p3, y_train_p3, y_test_p3 = train_test_split(X_p3, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler_p3 = MinMaxScaler()\n",
    "\n",
    "X_train_p3_scl = scaler_p3.fit_transform(X_train_p3)\n",
    "X_test_p3_scl = scaler_p3.transform(X_test_p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ee3ef916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Logistic Regression (aka logit, MaxEnt) classifier.\n",
      "\n",
      "    In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      "    scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
      "    cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      "    (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      "    'sag', 'saga' and 'newton-cg' solvers.)\n",
      "\n",
      "    This class implements regularized logistic regression using the\n",
      "    'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
      "    that regularization is applied by default**. It can handle both dense\n",
      "    and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
      "    floats for optimal performance; any other input format will be converted\n",
      "    (and copied).\n",
      "\n",
      "    The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      "    with primal formulation, or no regularization. The 'liblinear' solver\n",
      "    supports both L1 and L2 regularization, with a dual formulation only for\n",
      "    the L2 penalty. The Elastic-Net regularization is only supported by the\n",
      "    'saga' solver.\n",
      "\n",
      "    Read more in the :ref:`User Guide <logistic_regression>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n",
      "        Specify the norm of the penalty:\n",
      "\n",
      "        - `None`: no penalty is added;\n",
      "        - `'l2'`: add a L2 penalty term and it is the default choice;\n",
      "        - `'l1'`: add a L1 penalty term;\n",
      "        - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
      "\n",
      "        .. warning::\n",
      "           Some penalties may not work with some solvers. See the parameter\n",
      "           `solver` below, to know the compatibility between the penalty and\n",
      "           solver.\n",
      "\n",
      "        .. versionadded:: 0.19\n",
      "           l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      "\n",
      "    dual : bool, default=False\n",
      "        Dual (constrained) or primal (regularized, see also\n",
      "        :ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation\n",
      "        is only implemented for l2 penalty with liblinear solver. Prefer dual=False when\n",
      "        n_samples > n_features.\n",
      "\n",
      "    tol : float, default=1e-4\n",
      "        Tolerance for stopping criteria.\n",
      "\n",
      "    C : float, default=1.0\n",
      "        Inverse of regularization strength; must be a positive float.\n",
      "        Like in support vector machines, smaller values specify stronger\n",
      "        regularization.\n",
      "\n",
      "    fit_intercept : bool, default=True\n",
      "        Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "        added to the decision function.\n",
      "\n",
      "    intercept_scaling : float, default=1\n",
      "        Useful only when the solver 'liblinear' is used\n",
      "        and self.fit_intercept is set to True. In this case, x becomes\n",
      "        [x, self.intercept_scaling],\n",
      "        i.e. a \"synthetic\" feature with constant value equal to\n",
      "        intercept_scaling is appended to the instance vector.\n",
      "        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      "\n",
      "        Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      "        as all other features.\n",
      "        To lessen the effect of regularization on synthetic feature weight\n",
      "        (and therefore on the intercept) intercept_scaling has to be increased.\n",
      "\n",
      "    class_weight : dict or 'balanced', default=None\n",
      "        Weights associated with classes in the form ``{class_label: weight}``.\n",
      "        If not given, all classes are supposed to have weight one.\n",
      "\n",
      "        The \"balanced\" mode uses the values of y to automatically adjust\n",
      "        weights inversely proportional to class frequencies in the input data\n",
      "        as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "\n",
      "        Note that these weights will be multiplied with sample_weight (passed\n",
      "        through the fit method) if sample_weight is specified.\n",
      "\n",
      "        .. versionadded:: 0.17\n",
      "           *class_weight='balanced'*\n",
      "\n",
      "    random_state : int, RandomState instance, default=None\n",
      "        Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
      "        data. See :term:`Glossary <random_state>` for details.\n",
      "\n",
      "    solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n",
      "\n",
      "        Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
      "        To choose a solver, you might want to consider the following aspects:\n",
      "\n",
      "        - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
      "          and 'saga' are faster for large ones;\n",
      "        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
      "          'lbfgs' handle multinomial loss;\n",
      "        - 'liblinear' and 'newton-cholesky' can only handle binary classification\n",
      "          by default. To apply a one-versus-rest scheme for the multiclass setting\n",
      "          one can wrapt it with the `OneVsRestClassifier`.\n",
      "        - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n",
      "          especially with one-hot encoded categorical features with rare\n",
      "          categories. Be aware that the memory usage of this solver has a quadratic\n",
      "          dependency on `n_features` because it explicitly computes the Hessian\n",
      "          matrix.\n",
      "\n",
      "        .. warning::\n",
      "           The choice of the algorithm depends on the penalty chosen and on\n",
      "           (multinomial) multiclass support:\n",
      "\n",
      "           ================= ============================== ======================\n",
      "           solver            penalty                        multinomial multiclass\n",
      "           ================= ============================== ======================\n",
      "           'lbfgs'           'l2', None                     yes\n",
      "           'liblinear'       'l1', 'l2'                     no\n",
      "           'newton-cg'       'l2', None                     yes\n",
      "           'newton-cholesky' 'l2', None                     no\n",
      "           'sag'             'l2', None                     yes\n",
      "           'saga'            'elasticnet', 'l1', 'l2', None yes\n",
      "           ================= ============================== ======================\n",
      "\n",
      "        .. note::\n",
      "           'sag' and 'saga' fast convergence is only guaranteed on features\n",
      "           with approximately the same scale. You can preprocess the data with\n",
      "           a scaler from :mod:`sklearn.preprocessing`.\n",
      "\n",
      "        .. seealso::\n",
      "           Refer to the User Guide for more information regarding\n",
      "           :class:`LogisticRegression` and more specifically the\n",
      "           :ref:`Table <Logistic_regression>`\n",
      "           summarizing solver/penalty supports.\n",
      "\n",
      "        .. versionadded:: 0.17\n",
      "           Stochastic Average Gradient descent solver.\n",
      "        .. versionadded:: 0.19\n",
      "           SAGA solver.\n",
      "        .. versionchanged:: 0.22\n",
      "            The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
      "        .. versionadded:: 1.2\n",
      "           newton-cholesky solver.\n",
      "\n",
      "    max_iter : int, default=100\n",
      "        Maximum number of iterations taken for the solvers to converge.\n",
      "\n",
      "    multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
      "        If the option chosen is 'ovr', then a binary problem is fit for each\n",
      "        label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      "        across the entire probability distribution, *even when the data is\n",
      "        binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      "        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      "        and otherwise selects 'multinomial'.\n",
      "\n",
      "        .. versionadded:: 0.18\n",
      "           Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      "        .. versionchanged:: 0.22\n",
      "            Default changed from 'ovr' to 'auto' in 0.22.\n",
      "        .. deprecated:: 1.5\n",
      "           ``multi_class`` was deprecated in version 1.5 and will be removed in 1.7.\n",
      "           From then on, the recommended 'multinomial' will always be used for\n",
      "           `n_classes >= 3`.\n",
      "           Solvers that do not support 'multinomial' will raise an error.\n",
      "           Use `sklearn.multiclass.OneVsRestClassifier(LogisticRegression())` if you\n",
      "           still want to use OvR.\n",
      "\n",
      "    verbose : int, default=0\n",
      "        For the liblinear and lbfgs solvers set verbose to any positive\n",
      "        number for verbosity.\n",
      "\n",
      "    warm_start : bool, default=False\n",
      "        When set to True, reuse the solution of the previous call to fit as\n",
      "        initialization, otherwise, just erase the previous solution.\n",
      "        Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      "\n",
      "        .. versionadded:: 0.17\n",
      "           *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      "\n",
      "    n_jobs : int, default=None\n",
      "        Number of CPU cores used when parallelizing over classes if\n",
      "        multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
      "        set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      "        not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      "        context. ``-1`` means using all processors.\n",
      "        See :term:`Glossary <n_jobs>` for more details.\n",
      "\n",
      "    l1_ratio : float, default=None\n",
      "        The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
      "        used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
      "        to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
      "        to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
      "        combination of L1 and L2.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "\n",
      "    classes_ : ndarray of shape (n_classes, )\n",
      "        A list of class labels known to the classifier.\n",
      "\n",
      "    coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      "        Coefficient of the features in the decision function.\n",
      "\n",
      "        `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      "        In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
      "        to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
      "\n",
      "    intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      "        Intercept (a.k.a. bias) added to the decision function.\n",
      "\n",
      "        If `fit_intercept` is set to False, the intercept is set to zero.\n",
      "        `intercept_` is of shape (1,) when the given problem is binary.\n",
      "        In particular, when `multi_class='multinomial'`, `intercept_`\n",
      "        corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
      "        outcome 0 (False).\n",
      "\n",
      "    n_features_in_ : int\n",
      "        Number of features seen during :term:`fit`.\n",
      "\n",
      "        .. versionadded:: 0.24\n",
      "\n",
      "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "        Names of features seen during :term:`fit`. Defined only when `X`\n",
      "        has feature names that are all strings.\n",
      "\n",
      "        .. versionadded:: 1.0\n",
      "\n",
      "    n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
      "        Actual number of iterations for all classes. If binary or multinomial,\n",
      "        it returns only 1 element. For liblinear solver, only the maximum\n",
      "        number of iteration across all classes is given.\n",
      "\n",
      "        .. versionchanged:: 0.20\n",
      "\n",
      "            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      "            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    SGDClassifier : Incrementally trained logistic regression (when given\n",
      "        the parameter ``loss=\"log_loss\"``).\n",
      "    LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The underlying C implementation uses a random number generator to\n",
      "    select features when fitting the model. It is thus not uncommon,\n",
      "    to have slightly different results for the same input data. If\n",
      "    that happens, try with a smaller tol parameter.\n",
      "\n",
      "    Predict output may not match that of standalone liblinear in certain\n",
      "    cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      "    in the narrative documentation.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "\n",
      "    L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
      "        Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
      "        http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
      "\n",
      "    LIBLINEAR -- A Library for Large Linear Classification\n",
      "        https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      "\n",
      "    SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      "        Minimizing Finite Sums with the Stochastic Average Gradient\n",
      "        https://hal.inria.fr/hal-00860051/document\n",
      "\n",
      "    SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      "            :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n",
      "            for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n",
      "\n",
      "    Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      "        methods for logistic regression and maximum entropy models.\n",
      "        Machine Learning 85(1-2):41-75.\n",
      "        https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.datasets import load_iris\n",
      "    >>> from sklearn.linear_model import LogisticRegression\n",
      "    >>> X, y = load_iris(return_X_y=True)\n",
      "    >>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
      "    >>> clf.predict(X[:2, :])\n",
      "    array([0, 0])\n",
      "    >>> clf.predict_proba(X[:2, :])\n",
      "    array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
      "           [9.7...e-01, 2.8...e-02, ...e-08]])\n",
      "    >>> clf.score(X, y)\n",
      "    0.97...\n",
      "     A decision tree classifier.\n",
      "\n",
      "    Read more in the :ref:`User Guide <tree>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n",
      "        The function to measure the quality of a split. Supported criteria are\n",
      "        \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n",
      "        Shannon information gain, see :ref:`tree_mathematical_formulation`.\n",
      "\n",
      "    splitter : {\"best\", \"random\"}, default=\"best\"\n",
      "        The strategy used to choose the split at each node. Supported\n",
      "        strategies are \"best\" to choose the best split and \"random\" to choose\n",
      "        the best random split.\n",
      "\n",
      "    max_depth : int, default=None\n",
      "        The maximum depth of the tree. If None, then nodes are expanded until\n",
      "        all leaves are pure or until all leaves contain less than\n",
      "        min_samples_split samples.\n",
      "\n",
      "    min_samples_split : int or float, default=2\n",
      "        The minimum number of samples required to split an internal node:\n",
      "\n",
      "        - If int, then consider `min_samples_split` as the minimum number.\n",
      "        - If float, then `min_samples_split` is a fraction and\n",
      "          `ceil(min_samples_split * n_samples)` are the minimum\n",
      "          number of samples for each split.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for fractions.\n",
      "\n",
      "    min_samples_leaf : int or float, default=1\n",
      "        The minimum number of samples required to be at a leaf node.\n",
      "        A split point at any depth will only be considered if it leaves at\n",
      "        least ``min_samples_leaf`` training samples in each of the left and\n",
      "        right branches.  This may have the effect of smoothing the model,\n",
      "        especially in regression.\n",
      "\n",
      "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "        - If float, then `min_samples_leaf` is a fraction and\n",
      "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "          number of samples for each node.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for fractions.\n",
      "\n",
      "    min_weight_fraction_leaf : float, default=0.0\n",
      "        The minimum weighted fraction of the sum total of weights (of all\n",
      "        the input samples) required to be at a leaf node. Samples have\n",
      "        equal weight when sample_weight is not provided.\n",
      "\n",
      "    max_features : int, float or {\"sqrt\", \"log2\"}, default=None\n",
      "        The number of features to consider when looking for the best split:\n",
      "\n",
      "            - If int, then consider `max_features` features at each split.\n",
      "            - If float, then `max_features` is a fraction and\n",
      "              `max(1, int(max_features * n_features_in_))` features are considered at\n",
      "              each split.\n",
      "            - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "            - If \"log2\", then `max_features=log2(n_features)`.\n",
      "            - If None, then `max_features=n_features`.\n",
      "\n",
      "        Note: the search for a split does not stop until at least one\n",
      "        valid partition of the node samples is found, even if it requires to\n",
      "        effectively inspect more than ``max_features`` features.\n",
      "\n",
      "    random_state : int, RandomState instance or None, default=None\n",
      "        Controls the randomness of the estimator. The features are always\n",
      "        randomly permuted at each split, even if ``splitter`` is set to\n",
      "        ``\"best\"``. When ``max_features < n_features``, the algorithm will\n",
      "        select ``max_features`` at random at each split before finding the best\n",
      "        split among them. But the best found split may vary across different\n",
      "        runs, even if ``max_features=n_features``. That is the case, if the\n",
      "        improvement of the criterion is identical for several splits and one\n",
      "        split has to be selected at random. To obtain a deterministic behaviour\n",
      "        during fitting, ``random_state`` has to be fixed to an integer.\n",
      "        See :term:`Glossary <random_state>` for details.\n",
      "\n",
      "    max_leaf_nodes : int, default=None\n",
      "        Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      "        Best nodes are defined as relative reduction in impurity.\n",
      "        If None then unlimited number of leaf nodes.\n",
      "\n",
      "    min_impurity_decrease : float, default=0.0\n",
      "        A node will be split if this split induces a decrease of the impurity\n",
      "        greater than or equal to this value.\n",
      "\n",
      "        The weighted impurity decrease equation is the following::\n",
      "\n",
      "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "                                - N_t_L / N_t * left_impurity)\n",
      "\n",
      "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "\n",
      "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "        if ``sample_weight`` is passed.\n",
      "\n",
      "        .. versionadded:: 0.19\n",
      "\n",
      "    class_weight : dict, list of dict or \"balanced\", default=None\n",
      "        Weights associated with classes in the form ``{class_label: weight}``.\n",
      "        If None, all classes are supposed to have weight one. For\n",
      "        multi-output problems, a list of dicts can be provided in the same\n",
      "        order as the columns of y.\n",
      "\n",
      "        Note that for multioutput (including multilabel) weights should be\n",
      "        defined for each class of every column in its own dict. For example,\n",
      "        for four-class multilabel classification weights should be\n",
      "        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      "        [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      "\n",
      "        The \"balanced\" mode uses the values of y to automatically adjust\n",
      "        weights inversely proportional to class frequencies in the input data\n",
      "        as ``n_samples / (n_classes * np.bincount(y))``\n",
      "\n",
      "        For multi-output, the weights of each column of y will be multiplied.\n",
      "\n",
      "        Note that these weights will be multiplied with sample_weight (passed\n",
      "        through the fit method) if sample_weight is specified.\n",
      "\n",
      "    ccp_alpha : non-negative float, default=0.0\n",
      "        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "        subtree with the largest cost complexity that is smaller than\n",
      "        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "        :ref:`minimal_cost_complexity_pruning` for details.\n",
      "\n",
      "        .. versionadded:: 0.22\n",
      "\n",
      "    monotonic_cst : array-like of int of shape (n_features), default=None\n",
      "        Indicates the monotonicity constraint to enforce on each feature.\n",
      "          - 1: monotonic increase\n",
      "          - 0: no constraint\n",
      "          - -1: monotonic decrease\n",
      "\n",
      "        If monotonic_cst is None, no constraints are applied.\n",
      "\n",
      "        Monotonicity constraints are not supported for:\n",
      "          - multiclass classifications (i.e. when `n_classes > 2`),\n",
      "          - multioutput classifications (i.e. when `n_outputs_ > 1`),\n",
      "          - classifications trained on data with missing values.\n",
      "\n",
      "        The constraints hold over the probability of the positive class.\n",
      "\n",
      "        Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n",
      "\n",
      "        .. versionadded:: 1.4\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    classes_ : ndarray of shape (n_classes,) or list of ndarray\n",
      "        The classes labels (single output problem),\n",
      "        or a list of arrays of class labels (multi-output problem).\n",
      "\n",
      "    feature_importances_ : ndarray of shape (n_features,)\n",
      "        The impurity-based feature importances.\n",
      "        The higher, the more important the feature.\n",
      "        The importance of a feature is computed as the (normalized)\n",
      "        total reduction of the criterion brought by that feature.  It is also\n",
      "        known as the Gini importance [4]_.\n",
      "\n",
      "        Warning: impurity-based feature importances can be misleading for\n",
      "        high cardinality features (many unique values). See\n",
      "        :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "\n",
      "    max_features_ : int\n",
      "        The inferred value of max_features.\n",
      "\n",
      "    n_classes_ : int or list of int\n",
      "        The number of classes (for single output problems),\n",
      "        or a list containing the number of classes for each\n",
      "        output (for multi-output problems).\n",
      "\n",
      "    n_features_in_ : int\n",
      "        Number of features seen during :term:`fit`.\n",
      "\n",
      "        .. versionadded:: 0.24\n",
      "\n",
      "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "        Names of features seen during :term:`fit`. Defined only when `X`\n",
      "        has feature names that are all strings.\n",
      "\n",
      "        .. versionadded:: 1.0\n",
      "\n",
      "    n_outputs_ : int\n",
      "        The number of outputs when ``fit`` is performed.\n",
      "\n",
      "    tree_ : Tree instance\n",
      "        The underlying Tree object. Please refer to\n",
      "        ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
      "        :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
      "        for basic usage of these attributes.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    DecisionTreeRegressor : A decision tree regressor.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The default values for the parameters controlling the size of the trees\n",
      "    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "    unpruned trees which can potentially be very large on some data sets. To\n",
      "    reduce memory consumption, the complexity and size of the trees should be\n",
      "    controlled by setting those parameter values.\n",
      "\n",
      "    The :meth:`predict` method operates using the :func:`numpy.argmax`\n",
      "    function on the outputs of :meth:`predict_proba`. This means that in\n",
      "    case the highest predicted probabilities are tied, the classifier will\n",
      "    predict the tied class with the lowest index in :term:`classes_`.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "\n",
      "    .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      "\n",
      "    .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
      "           and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
      "\n",
      "    .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
      "           Learning\", Springer, 2009.\n",
      "\n",
      "    .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
      "           https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.datasets import load_iris\n",
      "    >>> from sklearn.model_selection import cross_val_score\n",
      "    >>> from sklearn.tree import DecisionTreeClassifier\n",
      "    >>> clf = DecisionTreeClassifier(random_state=0)\n",
      "    >>> iris = load_iris()\n",
      "    >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
      "    ...                             # doctest: +SKIP\n",
      "    ...\n",
      "    array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n",
      "            0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n",
      "     C-Support Vector Classification.\n",
      "\n",
      "    The implementation is based on libsvm. The fit time scales at least\n",
      "    quadratically with the number of samples and may be impractical\n",
      "    beyond tens of thousands of samples. For large datasets\n",
      "    consider using :class:`~sklearn.svm.LinearSVC` or\n",
      "    :class:`~sklearn.linear_model.SGDClassifier` instead, possibly after a\n",
      "    :class:`~sklearn.kernel_approximation.Nystroem` transformer or\n",
      "    other :ref:`kernel_approximation`.\n",
      "\n",
      "    The multiclass support is handled according to a one-vs-one scheme.\n",
      "\n",
      "    For details on the precise mathematical formulation of the provided\n",
      "    kernel functions and how `gamma`, `coef0` and `degree` affect each\n",
      "    other, see the corresponding section in the narrative documentation:\n",
      "    :ref:`svm_kernels`.\n",
      "\n",
      "    To learn how to tune SVC's hyperparameters, see the following example:\n",
      "    :ref:`sphx_glr_auto_examples_model_selection_plot_nested_cross_validation_iris.py`\n",
      "\n",
      "    Read more in the :ref:`User Guide <svm_classification>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    C : float, default=1.0\n",
      "        Regularization parameter. The strength of the regularization is\n",
      "        inversely proportional to C. Must be strictly positive. The penalty\n",
      "        is a squared l2 penalty. For an intuitive visualization of the effects\n",
      "        of scaling the regularization parameter C, see\n",
      "        :ref:`sphx_glr_auto_examples_svm_plot_svm_scale_c.py`.\n",
      "\n",
      "    kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf'\n",
      "        Specifies the kernel type to be used in the algorithm. If\n",
      "        none is given, 'rbf' will be used. If a callable is given it is used to\n",
      "        pre-compute the kernel matrix from data matrices; that matrix should be\n",
      "        an array of shape ``(n_samples, n_samples)``. For an intuitive\n",
      "        visualization of different kernel types see\n",
      "        :ref:`sphx_glr_auto_examples_svm_plot_svm_kernels.py`.\n",
      "\n",
      "    degree : int, default=3\n",
      "        Degree of the polynomial kernel function ('poly').\n",
      "        Must be non-negative. Ignored by all other kernels.\n",
      "\n",
      "    gamma : {'scale', 'auto'} or float, default='scale'\n",
      "        Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
      "\n",
      "        - if ``gamma='scale'`` (default) is passed then it uses\n",
      "          1 / (n_features * X.var()) as value of gamma,\n",
      "        - if 'auto', uses 1 / n_features\n",
      "        - if float, must be non-negative.\n",
      "\n",
      "        .. versionchanged:: 0.22\n",
      "           The default value of ``gamma`` changed from 'auto' to 'scale'.\n",
      "\n",
      "    coef0 : float, default=0.0\n",
      "        Independent term in kernel function.\n",
      "        It is only significant in 'poly' and 'sigmoid'.\n",
      "\n",
      "    shrinking : bool, default=True\n",
      "        Whether to use the shrinking heuristic.\n",
      "        See the :ref:`User Guide <shrinking_svm>`.\n",
      "\n",
      "    probability : bool, default=False\n",
      "        Whether to enable probability estimates. This must be enabled prior\n",
      "        to calling `fit`, will slow down that method as it internally uses\n",
      "        5-fold cross-validation, and `predict_proba` may be inconsistent with\n",
      "        `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\n",
      "\n",
      "    tol : float, default=1e-3\n",
      "        Tolerance for stopping criterion.\n",
      "\n",
      "    cache_size : float, default=200\n",
      "        Specify the size of the kernel cache (in MB).\n",
      "\n",
      "    class_weight : dict or 'balanced', default=None\n",
      "        Set the parameter C of class i to class_weight[i]*C for\n",
      "        SVC. If not given, all classes are supposed to have\n",
      "        weight one.\n",
      "        The \"balanced\" mode uses the values of y to automatically adjust\n",
      "        weights inversely proportional to class frequencies in the input data\n",
      "        as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "\n",
      "    verbose : bool, default=False\n",
      "        Enable verbose output. Note that this setting takes advantage of a\n",
      "        per-process runtime setting in libsvm that, if enabled, may not work\n",
      "        properly in a multithreaded context.\n",
      "\n",
      "    max_iter : int, default=-1\n",
      "        Hard limit on iterations within solver, or -1 for no limit.\n",
      "\n",
      "    decision_function_shape : {'ovo', 'ovr'}, default='ovr'\n",
      "        Whether to return a one-vs-rest ('ovr') decision function of shape\n",
      "        (n_samples, n_classes) as all other classifiers, or the original\n",
      "        one-vs-one ('ovo') decision function of libsvm which has shape\n",
      "        (n_samples, n_classes * (n_classes - 1) / 2). However, note that\n",
      "        internally, one-vs-one ('ovo') is always used as a multi-class strategy\n",
      "        to train models; an ovr matrix is only constructed from the ovo matrix.\n",
      "        The parameter is ignored for binary classification.\n",
      "\n",
      "        .. versionchanged:: 0.19\n",
      "            decision_function_shape is 'ovr' by default.\n",
      "\n",
      "        .. versionadded:: 0.17\n",
      "           *decision_function_shape='ovr'* is recommended.\n",
      "\n",
      "        .. versionchanged:: 0.17\n",
      "           Deprecated *decision_function_shape='ovo' and None*.\n",
      "\n",
      "    break_ties : bool, default=False\n",
      "        If true, ``decision_function_shape='ovr'``, and number of classes > 2,\n",
      "        :term:`predict` will break ties according to the confidence values of\n",
      "        :term:`decision_function`; otherwise the first class among the tied\n",
      "        classes is returned. Please note that breaking ties comes at a\n",
      "        relatively high computational cost compared to a simple predict.\n",
      "\n",
      "        .. versionadded:: 0.22\n",
      "\n",
      "    random_state : int, RandomState instance or None, default=None\n",
      "        Controls the pseudo random number generation for shuffling the data for\n",
      "        probability estimates. Ignored when `probability` is False.\n",
      "        Pass an int for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    class_weight_ : ndarray of shape (n_classes,)\n",
      "        Multipliers of parameter C for each class.\n",
      "        Computed based on the ``class_weight`` parameter.\n",
      "\n",
      "    classes_ : ndarray of shape (n_classes,)\n",
      "        The classes labels.\n",
      "\n",
      "    coef_ : ndarray of shape (n_classes * (n_classes - 1) / 2, n_features)\n",
      "        Weights assigned to the features (coefficients in the primal\n",
      "        problem). This is only available in the case of a linear kernel.\n",
      "\n",
      "        `coef_` is a readonly property derived from `dual_coef_` and\n",
      "        `support_vectors_`.\n",
      "\n",
      "    dual_coef_ : ndarray of shape (n_classes -1, n_SV)\n",
      "        Dual coefficients of the support vector in the decision\n",
      "        function (see :ref:`sgd_mathematical_formulation`), multiplied by\n",
      "        their targets.\n",
      "        For multiclass, coefficient for all 1-vs-1 classifiers.\n",
      "        The layout of the coefficients in the multiclass case is somewhat\n",
      "        non-trivial. See the :ref:`multi-class section of the User Guide\n",
      "        <svm_multi_class>` for details.\n",
      "\n",
      "    fit_status_ : int\n",
      "        0 if correctly fitted, 1 otherwise (will raise warning)\n",
      "\n",
      "    intercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n",
      "        Constants in decision function.\n",
      "\n",
      "    n_features_in_ : int\n",
      "        Number of features seen during :term:`fit`.\n",
      "\n",
      "        .. versionadded:: 0.24\n",
      "\n",
      "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "        Names of features seen during :term:`fit`. Defined only when `X`\n",
      "        has feature names that are all strings.\n",
      "\n",
      "        .. versionadded:: 1.0\n",
      "\n",
      "    n_iter_ : ndarray of shape (n_classes * (n_classes - 1) // 2,)\n",
      "        Number of iterations run by the optimization routine to fit the model.\n",
      "        The shape of this attribute depends on the number of models optimized\n",
      "        which in turn depends on the number of classes.\n",
      "\n",
      "        .. versionadded:: 1.1\n",
      "\n",
      "    support_ : ndarray of shape (n_SV)\n",
      "        Indices of support vectors.\n",
      "\n",
      "    support_vectors_ : ndarray of shape (n_SV, n_features)\n",
      "        Support vectors. An empty array if kernel is precomputed.\n",
      "\n",
      "    n_support_ : ndarray of shape (n_classes,), dtype=int32\n",
      "        Number of support vectors for each class.\n",
      "\n",
      "    probA_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n",
      "    probB_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n",
      "        If `probability=True`, it corresponds to the parameters learned in\n",
      "        Platt scaling to produce probability estimates from decision values.\n",
      "        If `probability=False`, it's an empty array. Platt scaling uses the\n",
      "        logistic function\n",
      "        ``1 / (1 + exp(decision_value * probA_ + probB_))``\n",
      "        where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\n",
      "        more information on the multiclass case and training procedure see\n",
      "        section 8 of [1]_.\n",
      "\n",
      "    shape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n",
      "        Array dimensions of training vector ``X``.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    SVR : Support Vector Machine for Regression implemented using libsvm.\n",
      "\n",
      "    LinearSVC : Scalable Linear Support Vector Machine for classification\n",
      "        implemented using liblinear. Check the See Also section of\n",
      "        LinearSVC for more comparison element.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] `LIBSVM: A Library for Support Vector Machines\n",
      "        <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n",
      "\n",
      "    .. [2] `Platt, John (1999). \"Probabilistic Outputs for Support Vector\n",
      "        Machines and Comparisons to Regularized Likelihood Methods\"\n",
      "        <https://citeseerx.ist.psu.edu/doc_view/pid/42e5ed832d4310ce4378c44d05570439df28a393>`_\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.pipeline import make_pipeline\n",
      "    >>> from sklearn.preprocessing import StandardScaler\n",
      "    >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
      "    >>> y = np.array([1, 1, 2, 2])\n",
      "    >>> from sklearn.svm import SVC\n",
      "    >>> clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
      "    >>> clf.fit(X, y)\n",
      "    Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                    ('svc', SVC(gamma='auto'))])\n",
      "\n",
      "    >>> print(clf.predict([[-0.8, -1]]))\n",
      "    [1]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "#Stage 2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Didnt know about doc strings super cool!\n",
    "logistic_regression_doc = LogisticRegression.__doc__\n",
    "decision_tree_doc = DecisionTreeClassifier.__doc__\n",
    "svc_doc = SVC.__doc__\n",
    "\n",
    "print(logistic_regression_doc, decision_tree_doc, svc_doc) #My version of jupyter is wierd it looks nicer printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "66c0e2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8591549295774648 0.9295774647887324 0.8990610328638498 0.9061032863849765 0.9178403755868545\n",
      "\n",
      "\n",
      "0.8591549295774648 0.92018779342723 0.9061032863849765\n",
      "\n",
      "\n",
      "0.9084507042253521 0.8990610328638498 0.9178403755868545\n"
     ]
    }
   ],
   "source": [
    "#Stage 3 (I know there is nothing to hand in here, but just messing around)\n",
    "# Importing necessary libraries for evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "log_reg = LogisticRegression(C=0.1)\n",
    "log_reg.fit(X_train_scl, y_train)\n",
    "log_reg_score = log_reg.score(X_test_scl, y_test)\n",
    "\n",
    "log_reg_tweaked = LogisticRegression(max_iter=1000, C=0.1)\n",
    "log_reg_tweaked.fit(X_train_scl, y_train)\n",
    "log_reg_tweaked_score = log_reg_tweaked.score(X_test_scl, y_test)\n",
    "\n",
    "log_reg_p3 = LogisticRegression(solver='liblinear', C=0.1)\n",
    "log_reg_p3.fit(X_train_p3_scl, y_train_p3)\n",
    "log_reg_p3_score = log_reg_p3.score(X_test_p3_scl, y_test_p3)\n",
    "\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf.fit(X_train_scl, y_train)\n",
    "tree_clf_score = tree_clf.score(X_test_scl, y_test)\n",
    "\n",
    "tree_clf_tweaked = DecisionTreeClassifier(max_depth=3)\n",
    "tree_clf_tweaked.fit(X_train_scl, y_train)\n",
    "tree_clf_tweaked_score = tree_clf_tweaked.score(X_test_scl, y_test)\n",
    "\n",
    "tree_clf_p3 = DecisionTreeClassifier()\n",
    "tree_clf_p3.fit(X_train_p3_scl, y_train_p3)\n",
    "tree_clf_p3_score = tree_clf_p3.score(X_test_p3_scl, y_test_p3)\n",
    "\n",
    "svc_clf = SVC()\n",
    "svc_clf.fit(X_train_scl, y_train)\n",
    "svc_clf_score = svc_clf.score(X_test_scl, y_test)\n",
    "\n",
    "svc_clf_tweaked = SVC(C=1.0, max_iter=1000)\n",
    "svc_clf_tweaked.fit(X_train_scl, y_train)\n",
    "svc_clf_tweaked_score = svc_clf_tweaked.score(X_test_scl, y_test)\n",
    "\n",
    "svc_clf_p3 = SVC()\n",
    "svc_clf_p3.fit(X_train_p3_scl, y_train_p3)\n",
    "svc_clf_p3_score = svc_clf_p3.score(X_test_p3_scl, y_test_p3)\n",
    "\n",
    "\n",
    "\n",
    "print(log_reg_score, tree_clf_score, tree_clf_p3_score, svc_clf_score, svc_clf_p3_score)\n",
    "print(\"\\n\")\n",
    "print(log_reg_tweaked_score, tree_clf_tweaked_score, svc_clf_tweaked_score)\n",
    "print(\"\\n\")\n",
    "print(log_reg_p3_score, tree_clf_p3_score, svc_clf_p3_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "30ead859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spot/College/Machine_Learning/machine-learning-labs/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/home/spot/College/Machine_Learning/machine-learning-labs/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "/home/spot/College/Machine_Learning/machine-learning-labs/lib/python3.11/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Scores on Scaled Data:\n",
      "         Train Score  Test Score\n",
      "C=10^-3     0.777647    0.781690\n",
      "C=10^-2     0.777647    0.781690\n",
      "C=10^-1     0.834706    0.859155\n",
      "C=10^0      0.887647    0.887324\n",
      "C=10^1      0.890588    0.875587\n",
      "C=10^2      0.889412    0.873239\n",
      "C=10^3      0.888235    0.873239\n",
      "\n",
      "SVC Scores on Scaled Data:\n",
      "         Train Score  Test Score\n",
      "C=10^-3     0.777647    0.781690\n",
      "C=10^-2     0.777647    0.781690\n",
      "C=10^-1     0.848824    0.877934\n",
      "C=10^0      0.887647    0.889671\n",
      "C=10^1      0.890588    0.882629\n",
      "C=10^2      0.890588    0.884977\n",
      "C=10^3      0.891176    0.884977\n",
      "\n",
      "Logistic Regression Scores on Polynomial Data:\n",
      "         Train Score  Test Score\n",
      "C=10^-3     0.777647    0.781690\n",
      "C=10^-2     0.832353    0.852113\n",
      "C=10^-1     0.895882    0.910798\n",
      "C=10^0      0.912941    0.910798\n",
      "C=10^1      0.927647    0.929577\n",
      "C=10^2      0.942941    0.931925\n",
      "C=10^3      0.953529    0.931925\n",
      "\n",
      "SVC Scores on Polynomial Data:\n",
      "         Train Score  Test Score\n",
      "C=10^-3     0.777647    0.781690\n",
      "C=10^-2     0.853529    0.877934\n",
      "C=10^-1     0.900588    0.913146\n",
      "C=10^0      0.920000    0.915493\n",
      "C=10^1      0.932941    0.927230\n",
      "C=10^2      0.950588    0.927230\n",
      "C=10^3      0.913529    0.910798\n"
     ]
    }
   ],
   "source": [
    "# Tables 1 and 2: Regularization comparison for Logistic Regression (LR) and Support Vector Classifier (SVC)\n",
    "# Trying different values of C = 10^n for both models\n",
    "\n",
    "n_values = [-3, -2, -1, 0, 1, 2, 3]  # Range of n values for C\n",
    "C_values = [10 ** n for n in n_values]  # Compute C values\n",
    "\n",
    "# Lists to store results for Logistic Regression and SVC on both regular and polynomial data\n",
    "lr_results_scl = []\n",
    "svc_results_scl = []\n",
    "lr_results_p3 = []\n",
    "svc_results_p3 = []\n",
    "\n",
    "for C in C_values:\n",
    "    # Logistic Regression on scaled data\n",
    "    lr_model = LogisticRegression(C=C, solver='lbfgs', max_iter=10000)  # Initialize model with C = 10^n\n",
    "    lr_model.fit(X_train_scl, y_train)  # Train the model\n",
    "    lr_train_score = lr_model.score(X_train_scl, y_train)  # Training score\n",
    "    lr_test_score = lr_model.score(X_test_scl, y_test)  # Testing score\n",
    "    lr_results_scl.append((lr_train_score, lr_test_score))  # Append the scores to the result list\n",
    "    \n",
    "    # SVC on scaled data\n",
    "    svc_model = SVC(C=C, kernel='linear', max_iter=10000)  # Initialize model with C = 10^n\n",
    "    svc_model.fit(X_train_scl, y_train)  # Train the model\n",
    "    svc_train_score = svc_model.score(X_train_scl, y_train)  # Training score\n",
    "    svc_test_score = svc_model.score(X_test_scl, y_test)  # Testing score\n",
    "    svc_results_scl.append((svc_train_score, svc_test_score))  # Append the scores to the result list\n",
    "    \n",
    "    # Logistic Regression on polynomial features data\n",
    "    lr_model_p3 = LogisticRegression(C=C, solver='lbfgs', max_iter=10000)  # Initialize model with C = 10^n\n",
    "    lr_model_p3.fit(X_train_p3_scl, y_train_p3)  # Train the model\n",
    "    lr_train_score_p3 = lr_model_p3.score(X_train_p3_scl, y_train_p3)  # Training score\n",
    "    lr_test_score_p3 = lr_model_p3.score(X_test_p3_scl, y_test_p3)  # Testing score\n",
    "    lr_results_p3.append((lr_train_score_p3, lr_test_score_p3))  # Append the scores to the result list\n",
    "    \n",
    "    # SVC on polynomial features data\n",
    "    svc_model_p3 = SVC(C=C, kernel='linear', max_iter=10000)  # Initialize model with C = 10^n\n",
    "    svc_model_p3.fit(X_train_p3_scl, y_train_p3)  # Train the model\n",
    "    svc_train_score_p3 = svc_model_p3.score(X_train_p3_scl, y_train_p3)  # Training score\n",
    "    svc_test_score_p3 = svc_model_p3.score(X_test_p3_scl, y_test_p3)  # Testing score\n",
    "    svc_results_p3.append((svc_train_score_p3, svc_test_score_p3))  # Append the scores to the result list\n",
    "\n",
    "# Create DataFrames to hold results\n",
    "columns = [f\"C=10^{n}\" for n in n_values]  # Column names for different C values\n",
    "\n",
    "# Create DataFrames for both regular and polynomial-transformed data\n",
    "lr_scl_df = pd.DataFrame(lr_results_scl, columns=[\"Train Score\", \"Test Score\"], index=columns)\n",
    "svc_scl_df = pd.DataFrame(svc_results_scl, columns=[\"Train Score\", \"Test Score\"], index=columns)\n",
    "lr_p3_df = pd.DataFrame(lr_results_p3, columns=[\"Train Score\", \"Test Score\"], index=columns)\n",
    "svc_p3_df = pd.DataFrame(svc_results_p3, columns=[\"Train Score\", \"Test Score\"], index=columns)\n",
    "\n",
    "# Display the results\n",
    "print(\"Logistic Regression Scores on Scaled Data:\")\n",
    "print(lr_scl_df)\n",
    "\n",
    "print(\"\\nSVC Scores on Scaled Data:\")\n",
    "print(svc_scl_df)\n",
    "\n",
    "\n",
    "# Questions:\n",
    "# - Which choice of model and regularization has the best test score?\n",
    "#   - The models appear to be acting roughly the same, but the higher the value of C the better the model appears to be acting.\n",
    "#   - I have tried to change the scaler but there appears to be problems with converging.\n",
    "# - Where does there seem to be overfitting of the training data?\n",
    "#   - There does not appear to be much ocerfitting of the data. At the same time, there appears to be low convergence for higher complex models\n",
    "#   - I theorize that this data set has low irriducible error.\n",
    "# - Where does there seem to be underfitting?\n",
    "#   - There also does not appear to be much underfitting. Although less complex models appear to not work as well, there is no great difference between training and testing scores.\n",
    "print(\"\\nLogistic Regression Scores on Polynomial Data:\")\n",
    "print(lr_p3_df)\n",
    "\n",
    "print(\"\\nSVC Scores on Polynomial Data:\")\n",
    "print(svc_p3_df)\n",
    "\n",
    "# Question:\n",
    "#  What is the overall best model out of your two tables?\n",
    "#     - The best overall model out of my two tables is Polynomial Logistic Regression with C=10^3\n",
    "\n",
    "\n",
    "#Code to create file\n",
    "file_name = 'model_comparison_tables_logistic_svc.tsv'\n",
    "tables = pd.concat([lr_scl_df, svc_scl_df, lr_p3_df, svc_p3_df])\n",
    "tables.to_csv(file_name, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "00a0e8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Scores on Scaled Data:\n",
      "                                                     max_depth=2  \\\n",
      "min_samples_split=0.01  (0.8694117647058823, 0.8967136150234741)   \n",
      "min_samples_split=0.1   (0.8694117647058823, 0.8967136150234741)   \n",
      "min_samples_split=0.2   (0.8505882352941176, 0.8732394366197183)   \n",
      "\n",
      "                                                     max_depth=4  \\\n",
      "min_samples_split=0.01  (0.9088235294117647, 0.9272300469483568)   \n",
      "min_samples_split=0.1   (0.8994117647058824, 0.9225352112676056)   \n",
      "min_samples_split=0.2   (0.8511764705882353, 0.8708920187793427)   \n",
      "\n",
      "                                                     max_depth=8  \\\n",
      "min_samples_split=0.01  (0.9505882352941176, 0.9225352112676056)   \n",
      "min_samples_split=0.1   (0.9117647058823529, 0.9272300469483568)   \n",
      "min_samples_split=0.2   (0.8511764705882353, 0.8708920187793427)   \n",
      "\n",
      "                                                    max_depth=16  \\\n",
      "min_samples_split=0.01  (0.9564705882352941, 0.9154929577464789)   \n",
      "min_samples_split=0.1   (0.9117647058823529, 0.9272300469483568)   \n",
      "min_samples_split=0.2   (0.8511764705882353, 0.8708920187793427)   \n",
      "\n",
      "                                                  max_depth=None  \n",
      "min_samples_split=0.01  (0.9564705882352941, 0.9154929577464789)  \n",
      "min_samples_split=0.1   (0.9117647058823529, 0.9272300469483568)  \n",
      "min_samples_split=0.2   (0.8511764705882353, 0.8708920187793427)  \n",
      "\n",
      "Decision Tree Scores on Polynomial Data:\n",
      "                                                    max_depth=2  \\\n",
      "min_samples_split=0.01  (0.861764705882353, 0.8802816901408451)   \n",
      "min_samples_split=0.1   (0.861764705882353, 0.8802816901408451)   \n",
      "min_samples_split=0.2   (0.861764705882353, 0.8802816901408451)   \n",
      "\n",
      "                                                     max_depth=4  \\\n",
      "min_samples_split=0.01  (0.9288235294117647, 0.9131455399061033)   \n",
      "min_samples_split=0.1   (0.9205882352941176, 0.9131455399061033)   \n",
      "min_samples_split=0.2   (0.8670588235294118, 0.8755868544600939)   \n",
      "\n",
      "                                                     max_depth=8  \\\n",
      "min_samples_split=0.01  (0.9588235294117647, 0.9084507042253521)   \n",
      "min_samples_split=0.1   (0.9311764705882353, 0.9178403755868545)   \n",
      "min_samples_split=0.2   (0.8717647058823529, 0.8732394366197183)   \n",
      "\n",
      "                                                    max_depth=16  \\\n",
      "min_samples_split=0.01  (0.9688235294117648, 0.9061032863849765)   \n",
      "min_samples_split=0.1   (0.9323529411764706, 0.9154929577464789)   \n",
      "min_samples_split=0.2   (0.8729411764705882, 0.8755868544600939)   \n",
      "\n",
      "                                                  max_depth=None  \n",
      "min_samples_split=0.01                (0.97, 0.9014084507042254)  \n",
      "min_samples_split=0.1   (0.9323529411764706, 0.9178403755868545)  \n",
      "min_samples_split=0.2   (0.8723529411764706, 0.8708920187793427)  \n"
     ]
    }
   ],
   "source": [
    "# Tables 3 and 4\n",
    "# Define values for min_samples_split and max_depth\n",
    "mss_values = [0.01, 0.1, 0.2]\n",
    "md_values = [2, 4, 8, 16, None]\n",
    "\n",
    "# Initialize result containers for the two tables\n",
    "tree_results_scl = []\n",
    "tree_results_p3 = []\n",
    "\n",
    "# Loop through min_samples_split (mss) and max_depth (md) for both the regular and polynomial data\n",
    "for mss in mss_values:\n",
    "    row_scl = []\n",
    "    row_p3 = []\n",
    "    for md in md_values:\n",
    "        # Decision Tree for scaled data\n",
    "        tree_clf_scl = DecisionTreeClassifier(min_samples_split=mss, max_depth=md)\n",
    "        tree_clf_scl.fit(X_train_scl, y_train)\n",
    "        train_score_scl = tree_clf_scl.score(X_train_scl, y_train)\n",
    "        test_score_scl = tree_clf_scl.score(X_test_scl, y_test)\n",
    "        row_scl.append((train_score_scl, test_score_scl))\n",
    "        \n",
    "        # Decision Tree for polynomial features data\n",
    "        tree_clf_p3 = DecisionTreeClassifier(min_samples_split=mss, max_depth=md)\n",
    "        tree_clf_p3.fit(X_train_p3_scl, y_train_p3)\n",
    "        train_score_p3 = tree_clf_p3.score(X_train_p3_scl, y_train_p3)\n",
    "        test_score_p3 = tree_clf_p3.score(X_test_p3_scl, y_test_p3)\n",
    "        row_p3.append((train_score_p3, test_score_p3))\n",
    "    \n",
    "    tree_results_scl.append(row_scl)\n",
    "    tree_results_p3.append(row_p3)\n",
    "\n",
    "# Create DataFrame for the results\n",
    "columns = [f\"max_depth={md}\" for md in md_values]\n",
    "index = [f\"min_samples_split={mss}\" for mss in mss_values]\n",
    "\n",
    "tree_scl_df = pd.DataFrame(tree_results_scl, columns=columns, index=index)\n",
    "tree_p3_df = pd.DataFrame(tree_results_p3, columns=columns, index=index)\n",
    "\n",
    "print(\"Decision Tree Scores on Scaled Data:\")\n",
    "print(tree_scl_df)\n",
    "\n",
    "# Questions:\n",
    "# - Which choice of model and regularization has the best test score?\n",
    "#   - The tree with the max_depth of 4 and min_samples_split of 0.1 appears to be acting the best\n",
    "#   - There is a trend that a higher min_samples split appears to be makeing the modle worse. Also an increasing max depth until 8 appears to make the model better.\n",
    "# - Where does there seem to be overfitting of the training data?\n",
    "#   - There does not appear to be much overfitting of the data.\n",
    "# - Where does there seem to be underfitting?\n",
    "#   - There also does not appear to be much underfitting.\n",
    "\n",
    "\n",
    "print(\"\\nDecision Tree Scores on Polynomial Data:\")\n",
    "print(tree_p3_df)\n",
    "\n",
    "# Question:\n",
    "#  What is the overall best model out of your two tables?\n",
    "#     - Many models are behaving similarly. However the trees with middle complexity appear to do best.\n",
    "\n",
    "# Assuming tree_scl_df and tree_p3_df are already defined as in the provided code\n",
    "\n",
    "# Saving the tables to an Excel file with different sheets for each table\n",
    "file_name = 'decision_tree_comparison_tables.tsv'\n",
    "tables = pd.concat([tree_scl_df, tree_p3_df])\n",
    "tables.to_csv(file_name, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5de36fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables 1 and 2\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "best_tree = DecisionTreeClassifier(min_samples_split=0.1, max_depth=8)\n",
    "best_tree.fit(X_train_scl, y_train)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(best_tree, feature_names=X.columns, filled=True, impurity=True, rounded=True)\n",
    "plt.title(\"Best Decision Tree - Overall (min_samples_split=0.01, max_depth=4)\")\n",
    "best_tree_file = 'best_decision_tree_overall.png'\n",
    "plt.savefig(best_tree_file, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "best_tree_md4 = DecisionTreeClassifier(min_samples_split=0.1, max_depth=4)\n",
    "best_tree_md4.fit(X_train_scl, y_train)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(best_tree_md4, feature_names=X.columns, filled=True, impurity=True, rounded=True)\n",
    "plt.title(\"Best Decision Tree with max_depth=4 (min_samples_split=0.01)\")\n",
    "best_tree_md4_file = 'best_decision_tree_md4.png'\n",
    "plt.savefig(best_tree_md4_file, dpi=300)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-labs",
   "language": "python",
   "name": "machine-learning-labs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
